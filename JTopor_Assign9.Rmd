---
title: "CUNY MSDA 605 Spring 2016 Assignment 9"
author: "James Topor"
output: html_document
---

# -----------------------------------------------------------------------------

__1.__ __*Write a function to produce a sample of a random variable distributed as:*__

$f(x) = x, 0 <= x <= 1$,

$f(x) = 2 - x, 1 < x <= 2$

For this problem we'll try to employ the inverse CDF (cumulative distribution function) sampling technique which allows us to sample from a uniform (0,1) distribution if we know the inverse of the CDF. 

Before we can employ the inverse CDF we must first derive the CDF from the PDF functions we've been given. To do that, we need to find the integral for each of the equations comprising the PDF. 

The integral of x = $x^2/2$.  
The integral of $2 - x$ = $2x - x^2/2 + 1$.  

Then we must find the inverse of each of these functions.

# -----

__For $x^2/2$ we have__:  

$F(x) = x^2/2 => y = x^2/2 => x = y^2/2 => 2x = y^2 => F^{-1}x = sqrt(2x)$

# -----

__For $2x - x^2/2 - 1$ we have__:

$F(x) = 2x - x^2/2 - 1$       =>  

$y = 2x - x^2/2 - 1$          =>  

$x = 2y - y^2/2 - 1$          =>  

$x = -(1/2)y^2 + 2y - 1$      =>  

$-x = (1/2)y^2 - 2y + 1$      =>  

$-2x = y^2 - 4y  + 2$         =>  

$-2x - 2 = y^2 - 4y$          =>

(Now we can complete the square on the right side)  
$-2x - 2 + (-4/2)^2 = y^2 - 4y + (-4/2)^2$ =>  

$-2x - 2 + 4 = y^2 - 4y + 4$  =>  

$2 - 2x = (y - 2)^2$          =>  

$(y - 2)^2 = 2 - 2x$          =>  

$y-2 = sqrt(2 - 2x)$          =>

$y = 2 +/- sqrt(2 - 2x)$      =>

$F^{-1} x = 2 +/- sqrt(2 - 2x)$  

Since 2 is our upper bound for the PDF, we will use $F^{-1}x = 2 - sqrt(2 - 2x)$ since this will restrict our generated values to within the specified range (i.e., less than 2).

Having found the inverses of the functions within the PDF, we can now define an R function that encapsulates their computations. The range of the respective PDF components is normalized to compensate for the need to sample randomly from a uniform (0,1) distribution further on in our analysis:

```{r}
invCDF <- function(y) {
  if (0 <= y && y <= .5) return(sqrt(2*y))
  
  if (.5 < y && y <= 1) return(2 - sqrt(2 - (2*y)))

  # if x doesn't satisfy either of the above, return zero
  return(0)
}

```

Now that we have the function defined, we can test it by randomly sampling from a uniform distribution and plotting the results:

```{r}
library(ggplot2)

# take 1000 random samples from a uniform distr and submit to invCDF function
invCDFsamples <- sapply(runif(1000), invCDF)

# move sample data into a dataframe for plotting
invCDF_df <- data.frame(invCDFsamples)

# Plot the sample data in a geometric histogram for viewing
ggplot(invCDF_df, aes(invCDFsamples)) + geom_histogram(binwidth=.05) + 
    ggtitle("Triangular PDF")
```

# -----------------------------------------------------------------------------
__2.__ __*Write a function to produce a sample of a random variable distributed as:*__

$f(x) = 1 - x, 0 <= x <= 1$, 

$f(x) = x - 1, 1 < x <= 2$

For this problem we'll make use of the Slice Sampling approach discussed during our fifth lecture in conjunction with a function that we'll create anew to model the PDF given above. The function that models the PDF uses a simple bounded approach. However, since we are using the Slice Sampling approach we need to return the __log__ of the respective PDF equations:  
```{r}
s_slice <- function(x) {
  if (0 <= x && x <= 1) return(log(1 - x))
  
  if (1 < x && x <= 2) return(log(x - 1))
  
  # if x doesn't satisfy either of the above, return zero
  return(0)
}
```

To test the __s_slice__ function, we'll use the __diversitree__ package's __mcmc__ function to generate a range of permissable values to pass to the function. Specifically, we'll limit the values we pass to it to being within the bounds specified by the PDF (0 <= x <= 2). Then we'll send that vector of values through the __s_slice__ function to calculate their PDF-specified probabilities.

```{r, message=FALSE, warning=FALSE}
library(diversitree)

set.seed(1)

# run f2 function 1000 times to get sample values
pdf_samples <- mcmc(s_slice, 0.0001, 1000, 1, lower=0.0001, upper=1.9999, print.every = 500)

# plot samples
pdfv_df <- data.frame(pdf_samples$pars)
ggplot(pdfv_df, aes(pdf_samples$par)) + geom_histogram(binwidth=.05) + 
    ggtitle("Inverted Triangular PDF")

```


# -----------------------------------------------------------------------------
__3.__ __*Draw 1000 samples from each of the above two distributions and plot the resulting histograms. You should have one histogram for each PDF. See that it matches your understanding of these PDFs.*__  

The requested histograms were plotted above in sections __1.__ and __2.__, where we tested the PDF functions __invCDF__ and __f2__ by drawing 1000 random samples from each function and plotting a histogram of the results. The plots do match my understanding of the PDF's. 

For example, in the plot in section __1.__ we see a triangular PDF where the values generated by the inverse CDF scale with x as x increases from 0 toward 1. This reflects our $f(x) = x, 0 <= x <= 1$ PDF. Then, as x increases from 1 to 2 we see the values generated by the inverse CDF decrease from 1 toward 0, which reflects our $f(x) = 2 - x, 1 < x <= 2$ PDF.  

For the plot in section __2.__ we see an inverted triangular PDF where the values generated by the inverse CDF decrease as x increases from 0 toward 1. This reflects our $f(x) = 1  - x, 0 <= x <= 1$ PDF. Then, as x increases from 1 to 2 we see the values generated by the inverse CDF increase from 0 toward 1, which reflects our $f(x) = x - 1, 1 < x <= 2$ PDF.  

# -----------------------------------------------------------------------------
__4.__ __*Now, write a program that will take a sample set size n as a parameter and the PDF as the second parameter, and perform 1000 iterations where it samples from the PDF, each time taking n samples and computes the mean of these n samples. It then plots a histogram of these 1000 means that it computes.*__

```{r}

# Function calculates means of 1000 samples from a given PDF and sample size
# A histogram is generated to plot the 1000 sample means
PDF_means <- function(n, pdf){
  
  # Generate a vector of values beteen 0 and 2
  pdf_vals <- seq(0,2, by = .01)

  # pass those values through the pdf function to get their probabilities
  pdfv_probs <- sapply(pdf_vals, pdf)
  
  # initialize a vector to be used for accumulating the sample means
  sMeans <- c()

  # Loop for 1000 iterations of sampling and computing means of samples
  for(i in 1:1000){
    # Take n samples of pdfv_probs values
    pdfv_samples<- sample(pdf_vals, n, replace = TRUE, prob = pdfv_probs)
    
    # Calculate mean of sample and append it to the sample means vector
    sMeans <- c(sMeans, mean(pdfv_samples))
  }
  # get the mean and stddev of the sample means
  mu <- mean(sMeans)
  stddev <- sd(sMeans)
  
  # plot sample means
  sMeans_df <- data.frame(sMeans)
  
  ggplot(sMeans_df, aes(sMeans)) + geom_histogram(binwidth=.05) + 
    xlab("Means of Samples") + ggtitle("")
    
}

# Test the PDF_means function using the invCDF function defined in Part 1 using n = 100
PDF_means(100, invCDF)

```

# -----------------------------------------------------------------------------

__5.__ __*Verify that as you set n to something like 10 or 20, each of the two PDFs pro- duce normally distributed mean of samples, empirically verifying the Central Limit Theorem. Please play around with various values of n and you'll see that even for reasonably small sample sizes such as 10, Central Limit Theorem holds.*__

Setting $n$ to something small like 10 or 20 does, in fact, produce a normally distributed mean of sample means as shown in the plots below. Additionally, we can see the maximum height of the distribution increasing while its spread decreases as $n$ becomes larger. As such, the Central Limit Theorem is verified.

```{r}
# Test the PDF_means function with n = 20
PDF_means(20, invCDF)

# Test the PDF_means function with n = 10
PDF_means(10, invCDF)

```