---
title: "CUNY MSDA 605 Spring 2016 Assignment 6"
author: "James Topor"
output: html_document
---

### Problem Set 1
# --------------------------------------------------------

__1.__ *When you roll a fair die 3 times, how many possible outcomes are there?*

If we are assuming the die is 6-sided, there are six possible outcomes for each roll of the die. As such, to find the number of possible outcomes from rolling that die 3 times we simply calculate 6 * 6 * 6, or $6^3$
```{r}
6^3
```

# --------------------------------------------------------
__2__. *What is the probability of getting a sum total of 3 when you roll a die 2 times?*

When rolling a single die twice there are $6 * 6 = 36$ possible outcomes. Of those 36, only 2 will yield a sum of 3: (2, 1) and (1, 2). As such, we have a probability of 2/36 of getting a sum total of 3 when rolling the die twice:
```{r}
2/36
```

As such, our probability is $0.055$

# --------------------------------------------------------
__3.__ *25 strangers in a room. What is the probability that 2 of them share the same birthday? Assume all birthdays are equally likely and have probability of 1/365.*

The probability of two people sharing a birthday can be found by first calculating the probability that no two of the 25 people share a birthday.  The probability of a pair of people not sharing a birthday is 364/365. 

As such, if we want to calculate the probability that no two of the 25 people share a birthday, we need to determine the number of possible pairs of people that can be created from the group of 25. In other words, how many ways are there to choose two people from a group of 25? 
```{r}
cPairs <- choose(25,2)
cPairs
```

Now that we know both the probability of a pair of people not sharing a birthday (364/365) AND the number of possible pairs of people we can create from a group of 25 (300), we simply multiply that probability by itself 300 times:  

```{r}
pNoShare <- (364/365)^cPairs
pNoShare
```

This result (0.439) is our probability of no 2 people of our group of 25 sharing a birthday, so to find the probability of 2 people from the group sharing a birthday we simply subtract that value from 1:
```{r}
pShare <- 1 - pNoShare
pShare
```

As such, our probability of two people from a group of 25 sharing a birthday is $0.5609$

When there are 50 people in the room, the probability increases substantially:

```{r}
cPairs <- choose(50,2)
pShare <- 1 - (364/365)^cPairs
pShare
```
We have a $0.96$ probability that two out of a group of 50 people will share a birthdate.

# --------------------------------------------------------
### Problem Set 2

*Write a program to take a document in English and print out the estimate probabilities of each word that occurs in that document. Please remove all punctuation and convert the words to lower case before performing the calculations.*

We'll start this task by using R's __scan__ function to read the contents of the text file into a list of strings. We then convert all elements of the list of strings to ASCII format, remove any punctuation or digits and convert all of the words to lowercase. 

We then make use of R's __table__ function to create a table containing a list of the unique words found within the list of strings we've created. The __table__ function also tallies up the frequencies with which each unique word occurs within the list of strings (and, hence, the document).

After converting that table to a data frame we sum the frequencies of each unique word to derive a count of the total number of words contained within the original text file. The word count is then used to calculate the required probabilities for each unique word: we simply divide the frequency with which each unique word occurs by the total word count to find a word's probability of occurring within the original text file. 

Finally, a list of all of the valid words from the original text file and their probabilities of occurrence within the text document are displayed.

```{r}
# load plyr library for 'arrange' function
library(plyr)

# Read the contents of the sample file into R using UTF-8 format
inDoc <- scan("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-605/master/assign6.sample.txt", 
              character(0), quote=NULL, encoding = "UTF-8") 

# Convert input doc text to ASCII to remove non-AsCII chars
stdText <- iconv(inDoc, "UTF-8", "ASCII", sub="")

# Remove any punctuation
stdText <- gsub("[[:punct:]]", "", stdText)
    
# Remove digits
stdText <- gsub("[0-9]", "", stdText)
    
# Remove any empty strings from the word list and convert all words to lowercase
stdText <- tolower(stdText[stdText != ''])

head(stdText)

# use R's table function to sum up occurrences of each word
probTable <- table(stdText)

# convert table into a dataframe
probList <- as.data.frame(probTable)
head(probList)

# get the total count of all words within the text by summing the frequenct counts of each unique word
wordCount <- sum(probList$Freq)
wordCount

# Now calculate the probability of each word's occurrence within the text by dividing its frequency by the
# total word count. The probability value gets appended to the existing 'probList' data frame.
probList$prob <- probList$Freq / wordCount

# Display a list of all of the words and their probabilities in alphabetical order
probList

```

The alphabetical list provided above can instead be sorted by word frequency if required:

```{r}
arrange(probList, desc(probList$Freq))
```

# ---------------------------------------------------------------
*Extend the program to calculate the probability of two words occurring adjacent to each other. The program should take in a document, and two words and compute the probability of each of the words occurring in the document and the joint probability of both of them occurring together. The order of the words is not important.*

If we assume that each word appears within the document independently of every other word in the document, and, as stated in the problem, the order of the words is not important, then the probability of, for example, "the cat" appearing within the text should be the same as the probability of "cat the" appearing within the text. When this is the case, the joint probability of two words occurring adjacent to each other is simply the probabilities of each individual word occurring within the text multiplied together. 

In order to create a self-contained function that accepts a document and two words, some of the R code developed above for the earlier part of the assignment is duplicated here and packaged into a reusable function.

```{r}

WordAdjacency <- function(inDoc, word1, word2){

  # make sure both words are lowercase
  word1 <- tolower(word1)
  word2 <- tolower(word2)
  
  # Read the contents of the 'inDoc' file into R
  Doc <- scan(inDoc, character(0), quote=NULL, encoding = "UTF-8") 
  
  # Convert input doc text to ASCII to remove non-AsCII chars
  stdText <- iconv(Doc, "UTF-8", "ASCII", sub="")

  # Remove any punctuation
  stdText <- gsub("[[:punct:]]", "", stdText)
    
  # Remove digits
  stdText <- gsub("[0-9]", "", stdText)
    
  # Remove any empty strings from the word list and convert all words to lowercase
  stdText <- tolower(stdText[stdText != ''])

  # use R's table function to sum up occurrences of each word
  probTable <- table(stdText)

  # convert table into a dataframe
  probList <- as.data.frame(probTable)

  # get the total count of all words within the text by summing the frequenct counts of each unique word
  wordCount <- sum(probList$Freq)

  # Now calculate the probability of each word by dividing its frequency by the total word count
  probList$prob <- probList$Freq / wordCount

  # get the individual probabilities for each of the input words
  pword1 <- 0
  pword2 <- 0
  pword1 <- probList$prob[probList$stdText == word1]
  pword2 <- probList$prob[probList$stdText == word2]
  
  # calculate the joint probability of the two words
  jProb <- pword1 * pword2

  return(list('ProbWord1'= pword1,'ProbWord2' = pword2, 'JointProb' = jProb))
}
```

Now that we have a reusable function defined we can test its functionality. Let's start by trying the words "prison" and "in"
```{r}
# test the function with 'prison' and 'in'
WordAdjacency("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-605/master/assign6.sample.txt",
              'prison', 'in')
```

The output of our function tells us that the probability of the word 'prison' is 0.008, the probability of the word 'in' is 0.0209, and their joint probability (the product of the two individual probabilities) is 0.000173.

Let's try inputting one word that's within the text ('corrections') and one that isn't ('fire'):
```{r}
# test the function with 'corrections' and 'fire'
WordAdjacency("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-605/master/assign6.sample.txt", 
              'corrections', 'fire')
```

The output of our function tells us that the probability of the word 'corrections' is 0.00749, the probability of the word 'fire' is zero, and their joint probability (the product of the two individual probabilities) is zero, which makes sense since the word 'fire' appears nowhere in the original text. 

Now let's try to articles that tend to occur relatively frequently in most texts, the words 'a' and 'the':
```{r}
# test the function with 'a' and 'the'
WordAdjacency("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-605/master/assign6.sample.txt", 
              'a', 'the')
```

The output of our function tells us that the probability of the word 'a' is 0.0337, the probability of the word 'the' is 0.0569, and their joint probability (the product of the two individual probabilities) is 0.00192.

# --------------------------------------------------------
*Compare your probabilities of various words with the Time Magazine corpus*

We can use the __plyr__ package's __arrange__ function to sort the list of words we've extracted from the text file by frequency of occurence. Once we've done that, we can list the top 20 words and check some of their probabilities against the Time Magazine corpus.

```{r}
top_words <- arrange(probList, desc(probList$Freq))
head(top_words, n=20)
```

Unsurprisingly, many of the most frequently occurring words are articles and conjunctions (e.g., a, and, but, etc). However, items 14 and 15 in our list ('prison' and 'corrections') give us an opportunity to compare their frequencies with those of the Time Magazine corpus.

For the most recent period available in that corpus (the decade of the 2000's) the word 'prison' had a frequency of 97.55 per million words, or

```{r}
round(97.55 / 1000000, 6)
```

which is less than the 0.008245877 frequency we found in our text file for that word.

The word 'corrections' had an even lower frequency of occurrence within the corpus during the 2000's: 4.36 per million words. This is less than the 0.007496252 frequency we found in our text file.

The word 'the', on the other hand, had a frequency of 50,590 per million words during the 2000's, or a little over 5%:
```{r}
round(50590 / 1000000, 6)
```

This isn't too far off from the nearly 5.7% frequency we found within our text file.

The word 'for' had a frequency of 8,879 per million words:
````{r}
round(8879/1000000, 6)
```

which is far lower than the 0.023238381 frequency we found within our text file.

These results demonstrate that context is of critical importance when attempting to quantify the frequentist probabilities of individual words within documents. An individual word might have a specific probability of occurrence within any given document, but its frequentist probability across a broader set of documents could be very different (either higher or lower) from that derived from any individual document.